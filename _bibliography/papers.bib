---
---


@InProceedings{pmlr-v235-pyakurel24a,
  title = 	 {Hierarchical Novelty Detection via Fine-Grained Evidence Allocation},
  author =       {Pyakurel, Spandan and Yu, Qi},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {41280--41302},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/pyakurel24a/pyakurel24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/pyakurel24a.html},
  abstract = 	 {By leveraging a hierarchical structure of known classes, Hierarchical Novelty Detection (HND) offers fine-grained detection results that pair detected novel samples with their closest (known) parent class in the hierarchy. Prior knowledge on the parent class provides valuable insights to better understand these novel samples. However, traditional novelty detection methods try to separate novel samples from all known classes using uncertainty or distance based metrics so they are incapable of locating the closest known parent class. Since the novel class is also part of the hierarchy, the model can more easily get confused between samples from known classes and those from novel ones. To achieve effective HND, we propose to augment the known (leaf-level) classes with a set of novel classes, each of which is associated with one parent (i.e., non-leaf) class in the original hierarchy. Such a structure allows us to perform novel fine-grained evidence allocation to differentiate known and novel classes guided by a uniquely designed loss function. Our thorough theoretical analysis shows that fine-grained evidence allocation creates an evidence margin to more precisely separate known and novel classes. Extensive experiments conducted on real-world hierarchical datasets demonstrate the proposed model outperforms the strongest baselines and achieves the best HND performance.}
}


@inproceedings{NEURIPS2024_4f1fbd5a,
 author = {Pandey, Deep and Pyakurel, Spandan and Yu, Qi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {44814--44844},
 publisher = {Curran Associates, Inc.},
 title = {Be Confident in What You Know: Bayesian Parameter Efficient Fine-Tuning of Vision Foundation Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/4f1fbd5ab8d58d0ecf33c95fd46b900e-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}
